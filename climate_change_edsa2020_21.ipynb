{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "climate-change-edsa2020-21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkhqPvwRTiOQ"
      },
      "source": [
        "# **1. Problem Statement and Importing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Qp0Shd9pOT",
        "outputId": "3253f2f8-854b-4141-8a4c-d9f6a09505d5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk import TreebankWordTokenizer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import urllib\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVQhP51hT1DI"
      },
      "source": [
        "# Import CSV files into kaggle notebook\n",
        "# sample_submission_data = pd.read_csv('../input/climate-change-edsa202021-data/sample_submission.csv')\n",
        "# test_data = pd.read_csv('../input/climate-change-edsa202021-data/test.csv')\n",
        "# train_data = pd.read_csv('../input/climate-change-edsa202021-data/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "knPo5VxX94us",
        "outputId": "501d97a0-e264-49a6-ce52-1c3effd28d27"
      },
      "source": [
        "# Import CSV files into colabs\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0420ec02-4f32-43d9-a6fd-092fe1b11f11\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0420ec02-4f32-43d9-a6fd-092fe1b11f11\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_submission.csv to sample_submission.csv\n",
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8kMVI1z_YXT"
      },
      "source": [
        "sample_submission_data = pd.read_csv('sample_submission.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "train_data = pd.read_csv('train.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS6AV2opSPX5"
      },
      "source": [
        "# **2. Tweets Preprocessing and Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDDc0gnVAkc"
      },
      "source": [
        "2.1 Make up of training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "2ylz5dxNS4bO",
        "outputId": "3df9d13f-bd2b-4b14-c1c6-fa257dba01c4"
      },
      "source": [
        "#Sample of the training data\n",
        "train_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid\n",
              "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
              "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "efVor5Rm9pOb",
        "outputId": "b09d7b6f-4e57-46fc-9d11-0f98db616a70"
      },
      "source": [
        "#Sample of tweet message in training data\n",
        "\n",
        "train_data['message'][10000]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The Washington Post LIES Non-Stop, like THIS: 'As Trump halts Fed action on climate change, cities &amp; states push on' https://t.co/4vOLbKiiLz\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHeDa5h39pOg",
        "outputId": "1a13194c-1f9b-4088-9fd9-46bc4c69c722"
      },
      "source": [
        "#Type of data for each column\n",
        "train_data.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15819 entries, 0 to 15818\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   sentiment  15819 non-null  int64 \n",
            " 1   message    15819 non-null  object\n",
            " 2   tweetid    15819 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 370.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrfOQ1b2VNH1"
      },
      "source": [
        "2.2 Preprocessing of training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsUtupGTe-9z"
      },
      "source": [
        "clean_data = train_data.copy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qPttWiQp9pOv"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPdJiqkG9pOx",
        "outputId": "0598b906-1f92-4da6-f2cb-b209e84a7def"
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'further', \"she's\", 'am', 'because', 'i', 'against', 'them', 'by', \"isn't\", 'an', 'why', 'yourself', 'have', 'too', 'very', 'as', \"couldn't\", \"didn't\", 'here', 'at', \"mustn't\", \"mightn't\", 'after', 'we', 'can', 'd', 'for', 'its', 'but', 'own', 'now', 'ain', 'shan', 'him', 'other', 'most', \"needn't\", 'your', 'off', 'which', 'don', 'were', 'itself', 'those', 'he', 'while', 'me', 'haven', 'has', 'there', 'didn', \"shan't\", 'll', 'same', \"haven't\", 'she', 'was', 'if', \"you'll\", 'our', 'before', 'or', 'himself', 'this', 'up', \"shouldn't\", 'you', \"aren't\", 'doing', 't', 'won', 'hadn', 'herself', 'couldn', 'their', 'ours', 'both', \"that'll\", 'should', 'hers', 'again', \"hadn't\", 'had', 'mightn', \"it's\", 'they', 'mustn', 'with', 'aren', \"wasn't\", 'few', 'these', 've', 'over', \"weren't\", 'of', 'who', 'a', 'some', 'be', 'his', 'needn', 'just', 'myself', 'and', 'o', 'hasn', 'not', 'did', 'than', 'above', 'having', 'where', 'been', 'the', 'yours', 'into', 'does', 'isn', 'once', 'wouldn', \"you've\", 'each', 'my', 'from', 'is', 'm', 'y', 'that', 're', \"wouldn't\", 'weren', 'do', 'such', 'until', 'in', 'then', 'what', 'about', \"hasn't\", 'yourselves', 'more', 'are', 'on', 'nor', \"you're\", 'ourselves', 'how', 'theirs', 'any', 'will', 'out', 'below', 'down', \"you'd\", 'her', 'whom', 'only', 'to', 'under', \"don't\", 'during', 'shouldn', 'so', 'ma', 's', 'themselves', 'doesn', 'through', 'wasn', 'all', 'being', \"should've\", 'no', 'when', \"won't\", 'it', \"doesn't\", 'between'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HItO5fki2AIL"
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Remove various features from tweet strings'''\n",
        "\n",
        "    text = str(text).lower() # Make lower case\n",
        "    text = re.sub('\\[.*?\\]', '', text) # Remove square brackets\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('@[^\\s]+', 'AT_USER', text) # Replace user names with 'AT_USER'\n",
        "    text = \"\".join([char.lower() for char in text if char not in string.punctuation]) # Reomove punctuation  \n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text) # Remove the # in #hashtag\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AuYI25IL9pOm"
      },
      "source": [
        "# Create new column with cleaned tweets\n",
        "# clean_data['clean'] = train_data['message'].apply(lambda x:clean_text(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYAmZYUUoCvf"
      },
      "source": [
        "# Skipped cleaning the tweet data because it made the model worse\n",
        "clean_data['clean'] = train_data['message']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Z-rCdDsb9pOu",
        "outputId": "944c7870-5bc7-4ec8-90e3-7d2bfef17ab7"
      },
      "source": [
        "clean_data.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                              clean\n",
              "0          1  ...  PolySciMajor EPA chief doesn't think carbon di...\n",
              "1          1  ...  It's not like we lack evidence of anthropogeni...\n",
              "2          2  ...  RT @RawStory: Researchers say we have three ye...\n",
              "3          1  ...  #TodayinMaker# WIRED : 2016 was a pivotal year...\n",
              "4          1  ...  RT @SoyNovioDeTodas: It's 2016, and a racist, ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vkwlek8Fmfg",
        "outputId": "d21d7fad-61df-472a-de49-8fad1164a6e7"
      },
      "source": [
        "clean_data['message'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    PolySciMajor EPA chief doesn't think carbon di...\n",
              "1    It's not like we lack evidence of anthropogeni...\n",
              "2    RT @RawStory: Researchers say we have three ye...\n",
              "3    #TodayinMaker# WIRED : 2016 was a pivotal year...\n",
              "4    RT @SoyNovioDeTodas: It's 2016, and a racist, ...\n",
              "Name: message, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x7wftq3j9pOz"
      },
      "source": [
        "#Function to tokenize tweet data\n",
        "def tokenize_column_data(df, column_name):\n",
        "  tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "  tweet_tokens = []\n",
        "  for index, value in clean_data[column_name].items():\n",
        "      \n",
        "      tweet_tokens.append(tweet_tokenizer.tokenize(value))\n",
        "\n",
        "  \n",
        "  df['tokenized'] = np.array(tweet_tokens)\n",
        "  df['tokenized'].apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n",
        "  \n",
        "  return df\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IauDiXy8teJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd70c0a-fffe-4426-f363-a6b646a3b7b4"
      },
      "source": [
        "# Create new column with tokenized tweet\n",
        "clean_data = tokenize_column_data(train_data, 'clean')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "kgAqaCyD_zig",
        "outputId": "532eb6b9-0ff4-4bda-a301-bef94e16688e"
      },
      "source": [
        "clean_data.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                          tokenized\n",
              "0          1  ...  [PolySciMajor, EPA, chief, doesn't, think, car...\n",
              "1          1  ...  [It's, not, like, we, lack, evidence, of, anth...\n",
              "2          2  ...  [RT, @RawStory, :, Researchers, say, we, have,...\n",
              "3          1  ...  [#TodayinMaker, #, WIRED, :, 2016, was, a, piv...\n",
              "4          1  ...  [RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n6ugKzx4ZM-"
      },
      "source": [
        "#Remove stopwords from tweets\n",
        "remove_stopwords = []\n",
        "for index, value in clean_data['tokenized'].items():\n",
        "  tokens_without_sw = [word for word in value if not word in stopwords.words()]\n",
        "      \n",
        "  remove_stopwords.append(tokens_without_sw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I60j2ejadRTf"
      },
      "source": [
        "#Add column to clean_data with removed stopwords\n",
        "#clean_data['nostopwords'] = np.array(remove_stopwords)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mUv59r8ovRy"
      },
      "source": [
        "# Did not include the remove stopwords step because it made the model worse\n",
        "clean_data['nostopwords'] = clean_data['tokenized']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "l2Z9lEwGdU78",
        "outputId": "6e3eac2c-8334-4860-d1a1-e40dc47af2f2"
      },
      "source": [
        "clean_data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>nostopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                        nostopwords\n",
              "0          1  ...  [PolySciMajor, EPA, chief, doesn't, think, car...\n",
              "1          1  ...  [It's, not, like, we, lack, evidence, of, anth...\n",
              "2          2  ...  [RT, @RawStory, :, Researchers, say, we, have,...\n",
              "3          1  ...  [#TodayinMaker, #, WIRED, :, 2016, was, a, piv...\n",
              "4          1  ...  [RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo6JWq3RQcAa"
      },
      "source": [
        "#Lemmatize tweet data\n",
        "lemma_list = []\n",
        "for index, value in clean_data['nostopwords'].items():\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  \n",
        "  lemma_tokens = [lemmatizer.lemmatize(w) for w in value]\n",
        "      \n",
        "  lemma_list.append(lemma_tokens)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5KULgG9ciBg"
      },
      "source": [
        "#Add column to clean_data with lemmatized words\n",
        "#clean_data['lemma'] = np.array(lemma_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8U6FxdFo-EM"
      },
      "source": [
        "# Skipped the lemmatization step becuase it made the model worse\n",
        "clean_data['lemma'] = clean_data['nostopwords']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "UkchBYqQcNAP",
        "outputId": "9d2d6165-1e43-48b3-8ed4-55589554073a"
      },
      "source": [
        "clean_data.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>nostopwords</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                              lemma\n",
              "0          1  ...  [PolySciMajor, EPA, chief, doesn't, think, car...\n",
              "1          1  ...  [It's, not, like, we, lack, evidence, of, anth...\n",
              "2          2  ...  [RT, @RawStory, :, Researchers, say, we, have,...\n",
              "3          1  ...  [#TodayinMaker, #, WIRED, :, 2016, was, a, piv...\n",
              "4          1  ...  [RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPms8d27pWde"
      },
      "source": [
        "#Concatinate tokenised sentence before vectorization\n",
        "concat_list = []\n",
        "for index, value in clean_data['nostopwords'].items():\n",
        "\n",
        "  concat = \" \".join(value)\n",
        "\n",
        "      \n",
        "  concat_list.append(concat)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-30MET0rEcV"
      },
      "source": [
        "#Add column to clean_data with concatinated sentence\n",
        "clean_data['concatenate'] = np.array(concat_list)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "eIyhnadgsM_h",
        "outputId": "950655ea-4686-4098-f0b8-ae684b38c2c1"
      },
      "source": [
        "clean_data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>nostopwords</th>\n",
              "      <th>lemma</th>\n",
              "      <th>concatenate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "      <td>[PolySciMajor, EPA, chief, doesn't, think, car...</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "      <td>[It's, not, like, we, lack, evidence, of, anth...</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "      <td>[RT, @RawStory, :, Researchers, say, we, have,...</td>\n",
              "      <td>RT @RawStory : Researchers say we have three y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "      <td>[#TodayinMaker, #, WIRED, :, 2016, was, a, piv...</td>\n",
              "      <td>#TodayinMaker # WIRED : 2016 was a pivotal yea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "      <td>[RT, @SoyNovioDeTodas, :, It's, 2016, ,, and, ...</td>\n",
              "      <td>RT @SoyNovioDeTodas : It's 2016 , and a racist...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                        concatenate\n",
              "0          1  ...  PolySciMajor EPA chief doesn't think carbon di...\n",
              "1          1  ...  It's not like we lack evidence of anthropogeni...\n",
              "2          2  ...  RT @RawStory : Researchers say we have three y...\n",
              "3          1  ...  #TodayinMaker # WIRED : 2016 was a pivotal yea...\n",
              "4          1  ...  RT @SoyNovioDeTodas : It's 2016 , and a racist...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og8jRowc3M4X"
      },
      "source": [
        "# The tweets did not need to be pre-processed because it made the model worse so fo the y data the original tweet 'message' was used\n",
        "y = clean_data['sentiment']\n",
        "X = clean_data['message']"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYi2MbWMcTeu"
      },
      "source": [
        "# Vectorize the tweet messages\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1, stop_words=\"english\")\n",
        "X_vectorized = vectorizer.fit_transform(X)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk-CUpQYFRhd"
      },
      "source": [
        "Class Description \n",
        "\n",
        "2 News: the tweet links to factual news about climate change \n",
        "\n",
        "1 Pro: the tweet supports the belief of man-made climate change \n",
        "\n",
        "0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change \n",
        "\n",
        "-1 Anti: the tweet does not believe in man-made climate change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wa80n-gqQoi",
        "outputId": "95cdea94-d30b-474d-fe3a-aae6fcf82443"
      },
      "source": [
        "# Imbalnced data shown below\n",
        "clean_data.sentiment.value_counts()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    8530\n",
              " 2    3640\n",
              " 0    2353\n",
              "-1    1296\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH-OAVDIY4LS",
        "outputId": "7294fb57-f124-450e-86c3-988104a77a21"
      },
      "source": [
        "# Use SMOTE to address the class imbalances in the data\n",
        "smote = SMOTE(sampling_strategy=\"not majority\")\n",
        "X_sm, y_sm = smote.fit_sample(X_vectorized, y)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6ZHisTPMgBj"
      },
      "source": [
        "# **3. Training the model and evaluating using the validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzTIA1fZx_ic"
      },
      "source": [
        "#Split data into train and test\n",
        "X_train,X_val,y_train,y_val = train_test_split(X_sm,y_sm,test_size=.3, random_state=11) #, shuffle=True"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqtGaV-myTOG"
      },
      "source": [
        "# Use a RandomForestClassifier as a model\n",
        "rfc = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
        "rfc.fit(X_train, y_train)\n",
        "rfc_pred = rfc.predict(X_val)"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSGQGJO4y2NP",
        "outputId": "3e55d10e-463f-4d29-9fc6-48cd08a11406"
      },
      "source": [
        "f1_score(y_val, rfc_pred, average=\"macro\")"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8927836709909829"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atCwpNysKycA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a3416c-51c9-4f90-bc26-446079fd1f89"
      },
      "source": [
        "# Try train a LogisticsRegression model\n",
        "lg = LogisticRegression()\n",
        "lg.fit(X_train, y_train)\n",
        "lg_pred = rfc.predict(X_val)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyCZDjXyI8b4",
        "outputId": "bb2a013f-8e10-44f4-910d-0eabc63bdb5a"
      },
      "source": [
        "f1_score(y_val, lg_pred, average=\"macro\")"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8927836709909829"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTtowfN48jT7"
      },
      "source": [
        "# **4. Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zikC0ZnYd_OG",
        "outputId": "22acbaf9-4fb8-436b-d5c3-f600cde0907c"
      },
      "source": [
        "print(classification_report(y_val, rfc_pred))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.99      0.94      0.97      2577\n",
            "           0       0.91      0.88      0.89      2549\n",
            "           1       0.79      0.85      0.82      2596\n",
            "           2       0.89      0.89      0.89      2514\n",
            "\n",
            "    accuracy                           0.89     10236\n",
            "   macro avg       0.90      0.89      0.89     10236\n",
            "weighted avg       0.89      0.89      0.89     10236\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH8XHcwKVOux"
      },
      "source": [
        "# **4. Hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDNbkDejHU7S"
      },
      "source": [
        "# Create a pipeline\n",
        "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n",
        "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
        "grid_param = [  {\"classifier\": [MultinomialNB()],\n",
        "                  \"classifier__alpha\": [1, 1e-1, 1e-2]\n",
        "                 },\n",
        "              \n",
        "                {\"classifier\": [AdaBoostClassifier()],\n",
        "                 'classifier__n_estimators': [50, 100],\n",
        "                 'classifier__learning_rate' : [0.01,0.05,0.1,0.3,1],\n",
        "                #  'classifier__loss' : ['linear', 'square', 'exponential']\n",
        "                },\n",
        "              \n",
        "                {\"classifier\": [LinearSVC()],\n",
        "                \"classifier__C\":[1, 10, 100],\n",
        "                # \"classifier__gamma\":[0.1, 0.01]\n",
        "                },               \n",
        "                {\"classifier\": [LogisticRegression()],\n",
        "                 \"classifier__penalty\": ['l2','l1'],\n",
        "                 \"classifier__C\": np.logspace(0, 4, 10)\n",
        "                 },\n",
        "                {\"classifier\": [LogisticRegression()],\n",
        "                 \"classifier__penalty\": ['l2'],\n",
        "                 \"classifier__C\": np.logspace(0, 4, 10),\n",
        "                 \"classifier__solver\":['newton-cg','saga','sag','liblinear'] # The solver does not allow for 'l1' penalty\n",
        "                 },\n",
        "                {\"classifier\": [RandomForestClassifier()],\n",
        "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
        "                 \"classifier__max_depth\":[5,8,15,25,30,None],\n",
        "                 \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n",
        "                 \"classifier__max_leaf_nodes\": [2, 5,10]}\n",
        "              ]\n",
        "\n",
        "gridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1, scoring='f1_macro') # Fit grid search\n",
        "best_model = gridsearch.fit(X_train,y_train) #Find the best model"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a0tqb5DToQ9",
        "outputId": "ba7cda96-c60b-4742-ce77-f99ccea8ad8f"
      },
      "source": [
        "#Check for the best model after hyperparameter tuning\n",
        "print(best_model.best_estimator_)\n",
        "print(\"The mean accuracy of the model is:\",best_model.score(X_val,y_val))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('classifier',\n",
            "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
            "                           fit_intercept=True, intercept_scaling=1,\n",
            "                           loss='squared_hinge', max_iter=1000,\n",
            "                           multi_class='ovr', penalty='l2', random_state=None,\n",
            "                           tol=0.0001, verbose=0))],\n",
            "         verbose=False)\n",
            "The mean accuracy of the model is: 0.9835109438700915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy594iwPUE2u"
      },
      "source": [
        "# Use the best model on the training data\n",
        "lm_best = LinearSVC(C=10, class_weight=None, dual=True,\n",
        "                           fit_intercept=True, intercept_scaling=1,\n",
        "                           loss='squared_hinge', max_iter=1000,\n",
        "                           multi_class='ovr', penalty='l2', random_state=None,\n",
        "                           tol=0.0001, verbose=0)\n",
        "lm_best.fit(X_train, y_train)\n",
        "lm_best_pred = rfc.predict(X_val)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgmM5Yg4UK41",
        "outputId": "d831d767-0a0f-4349-d4b6-853ae78aa951"
      },
      "source": [
        "#Check the f1 score\n",
        "f1_score(y_val, lm_best_pred, average=\"macro\")"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8927836709909829"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb6HBY6jVWr2",
        "outputId": "3c5fe009-a0c4-4f19-bea6-60948aebc4ac"
      },
      "source": [
        "print(classification_report(y_val, lm_best_pred))"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.99      0.94      0.97      2577\n",
            "           0       0.91      0.88      0.89      2549\n",
            "           1       0.79      0.85      0.82      2596\n",
            "           2       0.89      0.89      0.89      2514\n",
            "\n",
            "    accuracy                           0.89     10236\n",
            "   macro avg       0.90      0.89      0.89     10236\n",
            "weighted avg       0.89      0.89      0.89     10236\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffIvyQ1pFGHn"
      },
      "source": [
        "# **5. Making predictions on the test set and adding a sentiment column to our original test df**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b5_uVRci_XCa"
      },
      "source": [
        "testx = test_data['message']\n",
        "test_vect = vectorizer.transform(testx)"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeSrReR_FsO3"
      },
      "source": [
        "y_pred = lm_best.predict(test_vect)\n"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWPgAJXUFuox"
      },
      "source": [
        "test_data['sentiment'] = y_pred\n"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "prKUk1csFylJ",
        "outputId": "09740946-336a-4951-f46e-93b9a4cd6b52"
      },
      "source": [
        "test_data.head()\n"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europe will now be looking to China to make su...</td>\n",
              "      <td>169760</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Combine this with the polling of staffers re c...</td>\n",
              "      <td>35326</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The scary, unimpeachable evidence that climate...</td>\n",
              "      <td>224985</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
              "      <td>476263</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
              "      <td>872928</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message  tweetid  sentiment\n",
              "0  Europe will now be looking to China to make su...   169760          2\n",
              "1  Combine this with the polling of staffers re c...    35326          1\n",
              "2  The scary, unimpeachable evidence that climate...   224985          1\n",
              "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263          1\n",
              "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5_rOZ0wF1CI"
      },
      "source": [
        "test_data[['tweetid','sentiment']].to_csv('climate_change_edsa2020-21_submission.csv', index=False)\n"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "apmUJPMGF3RU",
        "outputId": "32d9d9d9-2f86-448d-e8dc-ac0861d7847e"
      },
      "source": [
        "files.download('climate_change_edsa2020-21_submission.csv')\n"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_43045bdb-968a-4673-a8f8-8ef31c37f1f0\", \"climate_change_edsa2020-21_submission.csv\", 94380)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}